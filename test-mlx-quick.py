#!/usr/bin/env python3
"""
Quick test of MLX with a small model
"""

import os
import sys

# Set cache directories
os.environ.setdefault('HF_HOME', '/Volumes/ExternalSSD/huggingface_cache')
os.environ.setdefault('TRANSFORMERS_CACHE', '/Volumes/ExternalSSD/huggingface_cache')

try:
    from mlx_lm import load, generate
    print("âœ… mlx-lm imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import mlx-lm: {e}")
    sys.exit(1)

def test_small_model():
    """Test with a smaller model for quick validation"""
    print("ğŸš€ Testing with a small model...")
    
    # Try a very small model that downloads quickly
    model_names = [
        "mlx-community/SmolLM-135M-Instruct-4bit",  # Tiny model
        "mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit",  # Small model
    ]
    
    for model_name in model_names:
        try:
            print(f"\nğŸ“¥ Trying {model_name}...")
            model, tokenizer = load(model_name)
            print(f"âœ… {model_name} loaded successfully")
            
            # Test generation
            prompt = "Hello! Write a short greeting:"
            print(f"ğŸ¯ Prompt: {prompt}")
            
            response = generate(
                model,
                tokenizer,
                prompt=prompt,
                max_tokens=50,
                temp=0.7
            )
            
            print(f"ğŸ¤– Response: {response}")
            print("âœ… MLX generation working!")
            return True
            
        except Exception as e:
            print(f"âš ï¸  {model_name} failed: {e}")
            continue
    
    return False

if __name__ == "__main__":
    print("ğŸ§ª Quick MLX Test...")
    
    if test_small_model():
        print("\nğŸ‰ MLX is working correctly!")
        print("ğŸ’¡ You can now use larger models like gpt-oss-20b-MLX-8bit")
    else:
        print("\nâŒ MLX test failed")
        print("ğŸ’¡ Check your MLX installation and cache permissions")