# Cortex OS consolidated environment template
# Generated: 2025-08-26T20:26:54Z
# NOTE: No secret values included. Populate locally or via CI/CD secrets.

# How to use: copy to .env, set values, see per-package README for details.

# ===== MCP OAuth Configuration =====
AUTH_MODE=oauth2
AUTH0_DOMAIN=your-tenant.auth0.com
AUTH0_AUDIENCE=https://your-mcp-resource
MCP_RESOURCE_URL=https://your-mcp-host/mcp
REQUIRED_SCOPES=search.read docs.write
REQUIRED_SCOPES_ENFORCE=true

# ===== Nx Cloud Configuration =====
# Get your access token from https://nx.app
NX_CLOUD_ACCESS_TOKEN=your-nx-cloud-access-token-here
NX_CLOUD_DISTRIBUTED_TASK_EXECUTION=true
NX_CLOUD_CACHE=true

# ===== brAInwav Production Standards =====
BRAINWAV_ENV=development
BRAINWAV_STRICT_MODE=false
BRAINWAV_TRACE_ENABLED=true

# sources: scripts/config/.env.1password
ANTHROPIC_API_KEY=

# sources: .env.cortex.template
API_KEY=

# sources: .env.cortex.template
API_URL=

# sources: .env.cortex.template
AUTH_TOKEN=

# sources: .env.cortex.template
BACKUP_DIR=

# sources: .env.local
BROWSER_USE_API_KEY=

# sources: .env.cortex.template
CACHE_DIR=

# sources: .env.carbon
CARBON_INTENSITY=

# sources: .env.carbon
CARBON_MEASUREMENT_INTERVAL=

# sources: .env.carbon
CARBON_MONITORING_ENABLED=

# sources: .env.carbon
CARBON_TARGET_PER_BUILD=

# sources: docs/examples/codex-cli/.env.example
CODEX_API_KEY=

# sources: docs/examples/codex-cli/.env.example
CODEX_DRY_RUN=

# sources: docs/examples/codex-cli/.env.example
CODEX_MAX_COST_USD=

# sources: docs/examples/codex-cli/.env.example
CODEX_MAX_TOKENS=

# sources: .env.template
CORTEX_CIPHER_URL=

# sources: docs/examples/codex-cli/.env.example, docs/examples/gemini-cli/.env.example
CORTEX_EXPERIMENTAL=

# sources: .env.cortex.template
CORTEX_MEMORIES_PATH=

# sources: .env.cortex.template, .env.example, scripts/config/.env.1password
DATABASE_URL=

# sources: .env.cortex.template
DB_HOST=

# sources: .env.cortex.template
DB_NAME=

# sources: .env.cortex.template
DB_PASSWORD=

# sources: .env.cortex.template
DB_PORT=

# sources: .env.cortex.template
DB_USER=

# sources: .env.example
DIRECT_URL=

# sources: docs/examples/gemini-cli/.env.example
GEMINI_API_KEY=

# sources: docs/examples/gemini-cli/.env.example
GEMINI_DRY_RUN=

# sources: docs/examples/gemini-cli/.env.example
GEMINI_MAX_COST_USD=

# sources: docs/examples/gemini-cli/.env.example
GEMINI_MAX_TOKENS=

# sources: scripts/config/.env.1password
GITHUB_TOKEN=

# sources: .env.cortex.template
GRAPHITI_GRAPH_PATH=

# sources: .env.cortex.template
GRAPHITI_INDEX_PATH=

# sources: .env.template
HF_HOME=

# sources: .env.template
HF_HUB_CACHE=

# sources: .env.template
HUGGINGFACE_HUB_CACHE=

# Hugging Face cache directory (for Docker deployments)
# Should point to external SSD for model persistence
HUGGINGFACE_CACHE_DIR=/Volumes/ExternalSSD/huggingface_cache

# sources: .env.cortex.template
JWT_SECRET=

# sources: .env.cortex.template
LETTA_CONTEXT_PATH=

# sources: .env.cortex.template
LETTA_MEMORY_PATH=

# sources: .env, scripts/.env
LINEAR_TOKEN=

# sources: .env.cortex.template, scripts/config/.env.1password
LOG_LEVEL=

# sources: .env.cortex.template
LOG_PATH=

# sources: .env.cortex.template
MEM0_CACHE_PATH=

# sources: .env.cortex.template
MEM0_STORAGE_PATH=

# sources: .env.cortex.template
MEMORY_INTEGRATION=

# sources: .env.cortex.template
MEMORY_LIMIT=

# sources: .env.cortex.template
MEMORY_MONITORING=
# Maximum offset for paginated memory queries
MEMORY_MAX_OFFSET=1000

# sources: .env.template
MLX_CACHE_DIR=/Volumes/ExternalSSD/huggingface_cache

# sources: .env.cortex.template
MLX_CONCURRENCY=3

# sources: .env.cortex.template
MLX_MANAGER_ENDPOINT=http://localhost:8765

# sources: .env.template
MLX_MODELS_DIR=/Volumes/ExternalSSD/ai-models

# sources: .env.cortex.template
MLX_PYTHON_SERVICE=

# sources: .env.cortex.template
MLX_SERVICE_URL=

# MLX Embeddings HTTP service (preferred)
# Set to the FastAPI service you run in services/py-mlx-server
# Leave blank to disable semantic/vector search.
MLX_EMBED_BASE_URL=

# --- Vector DB (Qdrant) Security Configuration ---
# For Docker-internal communication use http://qdrant:6333
QDRANT_URL=http://qdrant:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=local_memory_v1
QDRANT_TIMEOUT=5000
EMBED_DIM=384

# Memories embedder selection
# noop | mlx | ollama | composite
MEMORIES_EMBEDDER=

# MLX model selection for @cortex-os/memories
# qwen3-0.6b | qwen3-4b | qwen3-8b
MLX_MODEL=

# Ollama embedding API config (optional fallback)
# Leave blank to disable semantic/vector search or rely on MLX.
OLLAMA_BASE_URL=
OLLAMA_MODEL=

# sources: .env.example
NEXT_PUBLIC_SUPABASE_ANON_KEY=

# sources: .env.example
NEXT_PUBLIC_SUPABASE_URL=

# sources: .env.cortex.template, scripts/config/.env.1password
NODE_ENV=

# sources: .env.template
OLLAMA_MODELS=

# sources: .env.template
OMP_NUM_THREADS=

# sources: scripts/config/.env.1password
OPENAI_API_KEY=

# sources: .env.carbon
OTEL_ENDPOINT=

# sources: .env.cortex.template
PORT=

# Pieces OS MCP Integration
# Pieces OS MCP server port (default: 39300)
PIECES_MCP_PORT=39300
PIECES_MCP_ENDPOINT=http://localhost:39300/model_context_protocol/2024-11-05/sse
PIECES_MCP_ENABLED=true

# Cortex-OS MCP Server
CORTEX_MCP_PORT=3023

# Local Memory MCP Server
# Port 3024 is connected to Cloudflare tunnel: https://cortex-mcp.brainwav.io
MEMORY_MCP_PORT=3024
LOCAL_MEMORY_BASE_URL=http://localhost:3028/api/v1

# sources: scripts/config/.env.1password
PYTHON_ENV=

# sources: .env.template
PYTORCH_ENABLE_MPS_FALLBACK=

# sources: scripts/config/.env.1password
REDIS_URL=

# sources: .env.carbon
SCAPHANDRE_ENDPOINT=

# sources: .env.cortex.template
SESSION_SECRET=

# sources: scripts/config/.env.1password
SONAR_HOST_URL=

# sources: scripts/config/.env.1password
SONAR_TOKEN=

# sources: .env.example
SUPABASE_ANON_KEY=

# sources: .env.example
SUPABASE_SERVICE_ROLE_KEY=

# sources: .env.example
SUPABASE_URL=

# sources: .env.cortex.template
TEMP_DIR=

# sources: .env.cortex.template
UPLOAD_DIR=

# sources: scripts/config/.env.1password
VERTEX_AI_LOCATION=

# sources: scripts/config/.env.1password
VERTEX_AI_PROJECT_ID=


# --- Chat gateway (OpenAI-compatible) ---
# Provider selector for chat streaming backend.
# Supported values: 'openai' (real OpenAI), 'compatible' (OpenAI-compatible backends like Ollama/OpenRouter/Mistral/Groq), or leave empty to use local echo fallback.
MODEL_API_PROVIDER=

# Base URL for the OpenAI-compatible API when MODEL_API_PROVIDER is set (e.g., http://localhost:11434 for local providers, or https://api.openai.com)
MODEL_API_BASE=

# API key for the provider (optional for some local providers)
MODEL_API_KEY=

# ===== MCP Versioned Contracts Configuration =====
# MCP Notification Capabilities (all enabled by default for MCP compliance)
CORTEX_MCP_PROMPTS_LIST_CHANGED=true
CORTEX_MCP_RESOURCES_LIST_CHANGED=true
CORTEX_MCP_RESOURCES_SUBSCRIBE=true
CORTEX_MCP_TOOLS_LIST_CHANGED=true

# MCP Versioning Features
# Tools versioning is experimental (disabled by default)
CORTEX_MCP_TOOLS_VERSIONING=false
# Prompts and resources versioning are stable (enabled by default)
CORTEX_MCP_PROMPTS_VERSIONING=true
CORTEX_MCP_RESOURCES_VERSIONING=true

# MCP File System Watching
CORTEX_MCP_FS_WATCHER_ENABLED=true
CORTEX_MCP_FS_WATCHER_DEBOUNCE_MS=250
# Paths for watching prompts, resources, and tools
CORTEX_MCP_PROMPTS_PATH=prompts
CORTEX_MCP_RESOURCES_PATH=resources
CORTEX_MCP_TOOLS_PATH=tools

# MCP Manual Refresh Tool (always enabled for client compatibility)
CORTEX_MCP_MANUAL_REFRESH_ENABLED=true

# MCP Security and Performance Settings
CORTEX_MCP_ENABLE_SECURITY_VALIDATION=true
CORTEX_MCP_MAX_CONCURRENT_NOTIFICATIONS=10
CORTEX_MCP_NOTIFICATION_TIMEOUT_MS=5000

# ===== Vibe Check MCP Oversight (brAInwav) =====
# Vibe Check MCP server URL for CPI-based oversight
# Install: npx @pv-bhat/vibe-check-mcp start --http --port 2091
VIBE_CHECK_HTTP_URL=http://127.0.0.1:2091
# Enable/disable vibe check oversight (soft enforcement)
VIBE_CHECK_ENABLED=true

# MCP Development and Debugging
CORTEX_MCP_DEBUG_NOTIFICATIONS=false
CORTEX_MCP_LOG_NOTIFICATION_PAYLOADS=false

# ===== Ollama Integration for Pinned Behavior =====
# Ollama server configuration
OLLAMA_HOST=http://localhost:11434
# LLM backend selection (ollama|mlx)
CORTEX_LLM_BACKEND=ollama
# Ollama model configuration
CORTEX_OLLAMA_MODEL_TAG=llama3:latest
# Ollama Modelfile path for pinned behavior
CORTEX_OLLAMA_MODFILE_PATH=services/ollama/modelfiles/research.assistant.modelfile
# Force API overrides (system/template) from Modelfile content
CORTEX_OLLAMA_FORCE_API_OVERRIDES=false

# ===== MLX Integration for Pinned Behavior =====
# MLX configuration file for pinned behavior
CORTEX_MLX_CONFIG=services/mlx/configs/research.assistant.yaml
