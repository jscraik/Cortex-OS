# Cortex OS consolidated environment template
# Generated: 2025-08-26T20:26:54Z
# NOTE: No secret values included. Populate locally or via CI/CD secrets.

# How to use: copy to .env, set values, see per-package README for details.

# ===== Nx Cloud Configuration =====
# Get your access token from https://nx.app
NX_CLOUD_ACCESS_TOKEN=your-nx-cloud-access-token-here
NX_CLOUD_DISTRIBUTED_TASK_EXECUTION=true
NX_CLOUD_CACHE=true

# ===== brAInwav Production Standards =====
BRAINWAV_ENV=development
BRAINWAV_STRICT_MODE=false
BRAINWAV_TRACE_ENABLED=true

# sources: scripts/config/.env.1password
ANTHROPIC_API_KEY=

# sources: .env.cortex.template
API_KEY=

# sources: .env.cortex.template
API_URL=

# sources: .env.cortex.template
AUTH_TOKEN=

# sources: .env.cortex.template
BACKUP_DIR=

# sources: .env.local
BROWSER_USE_API_KEY=

# sources: .env.cortex.template
CACHE_DIR=

# sources: .env.carbon
CARBON_INTENSITY=

# sources: .env.carbon
CARBON_MEASUREMENT_INTERVAL=

# sources: .env.carbon
CARBON_MONITORING_ENABLED=

# sources: .env.carbon
CARBON_TARGET_PER_BUILD=

# sources: docs/examples/codex-cli/.env.example
CODEX_API_KEY=

# sources: docs/examples/codex-cli/.env.example
CODEX_DRY_RUN=

# sources: docs/examples/codex-cli/.env.example
CODEX_MAX_COST_USD=

# sources: docs/examples/codex-cli/.env.example
CODEX_MAX_TOKENS=

# sources: .env.template
CORTEX_CIPHER_URL=

# sources: docs/examples/codex-cli/.env.example, docs/examples/gemini-cli/.env.example
CORTEX_EXPERIMENTAL=

# sources: .env.cortex.template
CORTEX_MEMORIES_PATH=

# sources: .env.cortex.template, .env.example, scripts/config/.env.1password
DATABASE_URL=

# sources: .env.cortex.template
DB_HOST=

# sources: .env.cortex.template
DB_NAME=

# sources: .env.cortex.template
DB_PASSWORD=

# sources: .env.cortex.template
DB_PORT=

# sources: .env.cortex.template
DB_USER=

# sources: .env.example
DIRECT_URL=

# sources: docs/examples/gemini-cli/.env.example
GEMINI_API_KEY=

# sources: docs/examples/gemini-cli/.env.example
GEMINI_DRY_RUN=

# sources: docs/examples/gemini-cli/.env.example
GEMINI_MAX_COST_USD=

# sources: docs/examples/gemini-cli/.env.example
GEMINI_MAX_TOKENS=

# sources: scripts/config/.env.1password
GITHUB_TOKEN=

# sources: .env.cortex.template
GRAPHITI_GRAPH_PATH=

# sources: .env.cortex.template
GRAPHITI_INDEX_PATH=

# sources: .env.template
HF_HOME=

# sources: .env.template
HF_HUB_CACHE=

# sources: .env.template
HUGGINGFACE_HUB_CACHE=

# Hugging Face cache directory (for Docker deployments)
# Should point to external SSD for model persistence
HUGGINGFACE_CACHE_DIR=/Volumes/ExternalSSD/huggingface_cache

# sources: .env.cortex.template
JWT_SECRET=

# sources: .env.cortex.template
LETTA_CONTEXT_PATH=

# sources: .env.cortex.template
LETTA_MEMORY_PATH=

# sources: .env, scripts/.env
LINEAR_TOKEN=

# sources: .env.cortex.template, scripts/config/.env.1password
LOG_LEVEL=

# sources: .env.cortex.template
LOG_PATH=

# sources: .env.cortex.template
MEM0_CACHE_PATH=

# sources: .env.cortex.template
MEM0_STORAGE_PATH=

# sources: .env.cortex.template
MEMORY_INTEGRATION=

# sources: .env.cortex.template
MEMORY_LIMIT=

# sources: .env.cortex.template
MEMORY_MONITORING=
# Maximum offset for paginated memory queries
MEMORY_MAX_OFFSET=1000

# sources: .env.template
MLX_CACHE_DIR=/Volumes/ExternalSSD/huggingface_cache

# sources: .env.cortex.template
MLX_CONCURRENCY=3

# sources: .env.cortex.template
MLX_MANAGER_ENDPOINT=http://localhost:8765

# sources: .env.template
MLX_MODELS_DIR=/Volumes/ExternalSSD/ai-models

# sources: .env.cortex.template
MLX_PYTHON_SERVICE=

# sources: .env.cortex.template
MLX_SERVICE_URL=

# MLX Embeddings HTTP service (preferred)
# Set to the FastAPI service you run in services/py-mlx-server
# Leave blank to disable semantic/vector search.
MLX_EMBED_BASE_URL=

# --- Vector DB (Qdrant) Security Configuration ---
# For Docker-internal communication use http://qdrant:6333
QDRANT_URL=http://qdrant:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=local_memory_v1
QDRANT_TIMEOUT=5000
EMBED_DIM=384

# Memories embedder selection
# noop | mlx | ollama | composite
MEMORIES_EMBEDDER=

# MLX model selection for @cortex-os/memories
# qwen3-0.6b | qwen3-4b | qwen3-8b
MLX_MODEL=

# Ollama embedding API config (optional fallback)
# Leave blank to disable semantic/vector search or rely on MLX.
OLLAMA_BASE_URL=
OLLAMA_MODEL=

# sources: .env.example
NEXT_PUBLIC_SUPABASE_ANON_KEY=

# sources: .env.example
NEXT_PUBLIC_SUPABASE_URL=

# sources: .env.cortex.template, scripts/config/.env.1password
NODE_ENV=

# sources: .env.template
OLLAMA_MODELS=

# sources: .env.template
OMP_NUM_THREADS=

# sources: scripts/config/.env.1password
OPENAI_API_KEY=

# sources: .env.carbon
OTEL_ENDPOINT=

# sources: .env.cortex.template
PORT=

# Pieces OS MCP Integration
# Pieces OS MCP server port (default: 39300)
PIECES_MCP_PORT=39300
PIECES_MCP_ENDPOINT=http://localhost:39300/model_context_protocol/2024-11-05/sse
PIECES_MCP_ENABLED=true

# Cortex-OS MCP Server
CORTEX_MCP_PORT=3023

# Local Memory MCP Server
# Port 3024 is connected to Cloudflare tunnel: https://cortex-mcp.brainwav.io
MEMORY_MCP_PORT=3024
LOCAL_MEMORY_BASE_URL=http://localhost:3028/api/v1

# sources: scripts/config/.env.1password
PYTHON_ENV=

# sources: .env.template
PYTORCH_ENABLE_MPS_FALLBACK=

# sources: scripts/config/.env.1password
REDIS_URL=

# sources: .env.carbon
SCAPHANDRE_ENDPOINT=

# sources: .env.cortex.template
SESSION_SECRET=

# sources: scripts/config/.env.1password
SONAR_HOST_URL=

# sources: scripts/config/.env.1password
SONAR_TOKEN=

# sources: .env.example
SUPABASE_ANON_KEY=

# sources: .env.example
SUPABASE_SERVICE_ROLE_KEY=

# sources: .env.example
SUPABASE_URL=

# sources: .env.cortex.template
TEMP_DIR=

# sources: .env.cortex.template
UPLOAD_DIR=

# sources: scripts/config/.env.1password
VERTEX_AI_LOCATION=

# sources: scripts/config/.env.1password
VERTEX_AI_PROJECT_ID=


# --- Chat gateway (OpenAI-compatible) ---
# Provider selector for chat streaming backend.
# Supported values: 'openai' (real OpenAI), 'compatible' (OpenAI-compatible backends like Ollama/OpenRouter/Mistral/Groq), or leave empty to use local echo fallback.
MODEL_API_PROVIDER=

# Base URL for the OpenAI-compatible API when MODEL_API_PROVIDER is set (e.g., http://localhost:11434 for local providers, or https://api.openai.com)
MODEL_API_BASE=

# API key for the provider (optional for some local providers)
MODEL_API_KEY=
