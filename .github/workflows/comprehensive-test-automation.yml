name: Comprehensive Test Automation Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run nightly comprehensive tests
    - cron: '0 2 * * *'

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.12'
  PNPM_VERSION: '10.13.1'

jobs:
  # Unit Tests - Frontend (TypeScript/React)
  unit-tests-frontend:
    name: Frontend Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: Run Frontend Unit Tests
        run: |
          pnpm test:unit:frontend --coverage --reporter=json --outputFile=test-results/unit-frontend.json
        env:
          NODE_ENV: test

      - name: Upload Frontend Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: frontend-unit-test-results
          path: |
            test-results/unit-frontend.json
            coverage/frontend/

      - name: Comment Test Results on PR
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            try {
              const results = JSON.parse(fs.readFileSync('test-results/unit-frontend.json', 'utf8'));
              const passed = results.numPassedTests;
              const failed = results.numFailedTests;
              const coverage = results.coverageMap?.total?.lines?.pct || 0;
              
              const comment = `## ðŸ§ª Frontend Unit Tests Results
              
              - âœ… Passed: ${passed}
              - âŒ Failed: ${failed}
              - ðŸ“Š Coverage: ${coverage.toFixed(1)}%
              
              ${failed > 0 ? 'âš ï¸ Some tests failed. Please review the failures.' : 'ðŸŽ‰ All tests passed!'}`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not parse test results:', error);
            }

  # Unit Tests - Backend (Python)
  unit-tests-backend:
    name: Backend Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Setup uv
        uses: astral-sh/setup-uv@v4

      - name: Install Python Dependencies
        working-directory: apps/cortex-py
        run: |
          uv sync --all-extras
          uv pip install pytest-json-report

      - name: Run Backend Unit Tests
        working-directory: apps/cortex-py
        run: |
          uv run pytest tests/ \
            --cov=src \
            --cov-report=html \
            --cov-report=json \
            --json-report \
            --json-report-file=../../test-results/unit-backend.json

      - name: Upload Backend Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: backend-unit-test-results
          path: |
            test-results/unit-backend.json
            apps/cortex-py/htmlcov/

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [unit-tests-frontend, unit-tests-backend]
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js and Python
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Dependencies
        run: |
          pnpm install --frozen-lockfile
          cd apps/cortex-py && uv sync --all-extras

      - name: Start API Server
        run: |
          pnpm --filter @cortex-os/api dev &
          sleep 10
          curl --fail http://localhost:4000/health
        env:
          DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
          REDIS_URL: redis://localhost:6379

      - name: Run Integration Tests
        run: |
          pnpm test:integration --reporter=json --outputFile=test-results/integration.json
        env:
          DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
          REDIS_URL: redis://localhost:6379
          API_BASE_URL: http://localhost:4000

      - name: Upload Integration Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: test-results/integration.json

  # End-to-End Tests
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [integration-tests]
    
    strategy:
      matrix:
        browser: [chromium, firefox, webkit]
        
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: Install Playwright
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: Start Application
        run: |
          pnpm dev &
          npx wait-on http://localhost:3000 --timeout 60000

      - name: Run E2E Tests
        run: |
          npx playwright test --project=${{ matrix.browser }} \
            --reporter=json \
            --output-dir=test-results/e2e-${{ matrix.browser }}
        env:
          BASE_URL: http://localhost:3000

      - name: Upload E2E Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results-${{ matrix.browser }}
          path: |
            test-results/e2e-${{ matrix.browser }}/
            test-results/playwright-report/

  # Accessibility Tests
  accessibility-tests:
    name: Accessibility Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: Install Playwright and axe
        run: |
          npx playwright install --with-deps chromium
          npm install -g @axe-core/cli pa11y-ci

      - name: Start Application
        run: |
          pnpm dev &
          npx wait-on http://localhost:3000 --timeout 60000

      - name: Run WCAG Compliance Tests
        run: |
          npx playwright test tests/automation/accessibility/ \
            --reporter=json \
            --output-dir=test-results/accessibility
        env:
          BASE_URL: http://localhost:3000

      - name: Run Pa11y Accessibility Audit
        run: |
          pa11y-ci --sitemap http://localhost:3000/sitemap.xml \
            --standard WCAG2AA \
            --reporter json > test-results/pa11y-results.json
        continue-on-error: true

      - name: Run axe CLI Accessibility Scan
        run: |
          axe http://localhost:3000 \
            --tags wcag2a,wcag2aa,wcag21a,wcag21aa,wcag22a,wcag22aa \
            --reporter json \
            --output test-results/axe-results.json
        continue-on-error: true

      - name: Upload Accessibility Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accessibility-test-results
          path: test-results/

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Dependencies
        run: |
          pnpm install --frozen-lockfile
          npm install -g k6 @lhci/cli

      - name: Start Application
        run: |
          pnpm dev &
          npx wait-on http://localhost:3000 --timeout 60000

      - name: Run Lighthouse CI
        run: |
          lhci autorun
        env:
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}
          BASE_URL: http://localhost:3000

      - name: Run Load Tests with k6
        run: |
          k6 run tests/automation/performance/load-test.js \
            --env BASE_URL=http://localhost:3000 \
            --env K6_REPORT_PATH=test-results/k6-report.html \
            --env K6_JSON_PATH=test-results/k6-results.json

      - name: Upload Performance Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            test-results/k6-*
            .lighthouseci/

  # Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    timeout-minutes: 25
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: Run Security Tests
        run: |
          pnpm test:security --reporter=json --outputFile=test-results/security.json

      - name: Run npm audit
        run: |
          pnpm audit --audit-level moderate --json > test-results/npm-audit.json
        continue-on-error: true

      - name: Run Snyk Security Scan
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --json > test-results/snyk-results.json
        continue-on-error: true

      - name: Upload Security Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-test-results
          path: test-results/

  # Visual Regression Tests
  visual-regression-tests:
    name: Visual Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: Install Playwright
        run: npx playwright install --with-deps chromium

      - name: Start Application
        run: |
          pnpm dev &
          npx wait-on http://localhost:3000 --timeout 60000

      - name: Run Visual Regression Tests
        run: |
          npx playwright test tests/automation/visual/ \
            --update-snapshots \
            --reporter=json \
            --output-dir=test-results/visual
        env:
          BASE_URL: http://localhost:3000

      - name: Upload Visual Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: visual-regression-results
          path: test-results/visual/

  # Mobile Tests
  mobile-tests:
    name: Mobile Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        device: ['iPhone 12', 'Pixel 5', 'iPad Pro']
        
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: Install Playwright
        run: npx playwright install --with-deps chromium

      - name: Start Application
        run: |
          pnpm dev &
          npx wait-on http://localhost:3000 --timeout 60000

      - name: Run Mobile Tests
        run: |
          npx playwright test tests/automation/e2e/ \
            --project=mobile \
            --grep="${{ matrix.device }}" \
            --reporter=json \
            --output-dir=test-results/mobile-${{ matrix.device }}
        env:
          BASE_URL: http://localhost:3000
          DEVICE: ${{ matrix.device }}

      - name: Upload Mobile Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mobile-test-results-${{ matrix.device }}
          path: test-results/mobile-${{ matrix.device }}/

  # Quality Gates and Release Criteria
  quality-gates:
    name: Quality Gates Validation
    runs-on: ubuntu-latest
    needs: [
      unit-tests-frontend,
      unit-tests-backend,
      integration-tests,
      e2e-tests,
      accessibility-tests,
      performance-tests,
      security-tests
    ]
    if: always()
    
    steps:
      - name: Download All Test Results
        uses: actions/download-artifact@v4
        with:
          path: all-test-results/

      - name: Quality Gates Analysis
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const qualityGates = {
              unitTestCoverage: { threshold: 95, actual: 0, passed: false },
              e2eTestsPassed: { threshold: 100, actual: 0, passed: false },
              accessibilityScore: { threshold: 95, actual: 0, passed: false },
              performanceScore: { threshold: 90, actual: 0, passed: false },
              securityIssues: { threshold: 0, actual: 0, passed: false }
            };
            
            // Analyze test results
            const testResultsDir = 'all-test-results';
            const results = fs.readdirSync(testResultsDir);
            
            // Process each result type
            results.forEach(resultDir => {
              const resultPath = path.join(testResultsDir, resultDir);
              if (fs.statSync(resultPath).isDirectory()) {
                console.log(`Processing ${resultDir}...`);
                // Add specific parsing logic for each test type
              }
            });
            
            // Generate quality gates report
            const overallPassed = Object.values(qualityGates).every(gate => gate.passed);
            
            const report = `## ðŸŽ¯ Quality Gates Report
            
            | Gate | Threshold | Actual | Status |
            |------|-----------|--------|--------|
            | Unit Test Coverage | ${qualityGates.unitTestCoverage.threshold}% | ${qualityGates.unitTestCoverage.actual}% | ${qualityGates.unitTestCoverage.passed ? 'âœ…' : 'âŒ'} |
            | E2E Tests | ${qualityGates.e2eTestsPassed.threshold}% | ${qualityGates.e2eTestsPassed.actual}% | ${qualityGates.e2eTestsPassed.passed ? 'âœ…' : 'âŒ'} |
            | Accessibility | ${qualityGates.accessibilityScore.threshold}% | ${qualityGates.accessibilityScore.actual}% | ${qualityGates.accessibilityScore.passed ? 'âœ…' : 'âŒ'} |
            | Performance | ${qualityGates.performanceScore.threshold}% | ${qualityGates.performanceScore.actual}% | ${qualityGates.performanceScore.passed ? 'âœ…' : 'âŒ'} |
            | Security Issues | ${qualityGates.securityIssues.threshold} | ${qualityGates.securityIssues.actual} | ${qualityGates.securityIssues.passed ? 'âœ…' : 'âŒ'} |
            
            **Overall Status: ${overallPassed ? 'âœ… PASSED' : 'âŒ FAILED'}**
            
            ${overallPassed ? 
              'ðŸŽ‰ All quality gates passed! Ready for deployment.' : 
              'âš ï¸ Some quality gates failed. Please review and fix issues before deployment.'
            }`;
            
            // Create job summary
            core.summary.addRaw(report);
            await core.summary.write();
            
            // Fail the job if quality gates don't pass
            if (!overallPassed) {
              core.setFailed('Quality gates validation failed');
            }

  # Test Report Generation
  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [quality-gates]
    if: always()
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download All Test Results
        uses: actions/download-artifact@v4
        with:
          path: all-test-results/

      - name: Generate Comprehensive Test Report
        run: |
          mkdir -p test-reports
          
          # Create HTML report combining all test results
          cat > test-reports/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>Cortex OS - Test Automation Report</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 20px; }
                  .header { background: #f5f5f5; padding: 20px; border-radius: 5px; }
                  .section { margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }
                  .passed { color: green; }
                  .failed { color: red; }
                  .summary { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px; margin: 20px 0; }
                  .card { padding: 15px; border: 1px solid #ddd; border-radius: 5px; text-align: center; }
                  table { width: 100%; border-collapse: collapse; margin: 10px 0; }
                  th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }
                  th { background-color: #f2f2f2; }
              </style>
          </head>
          <body>
              <div class="header">
                  <h1>ðŸ§ª Cortex OS Test Automation Report</h1>
                  <p>Generated on: $(date)</p>
                  <p>Commit: ${{ github.sha }}</p>
                  <p>Branch: ${{ github.ref_name }}</p>
              </div>
              
              <div class="summary">
                  <div class="card">
                      <h3>Unit Tests</h3>
                      <p>Frontend & Backend</p>
                  </div>
                  <div class="card">
                      <h3>Integration Tests</h3>
                      <p>API & Services</p>
                  </div>
                  <div class="card">
                      <h3>E2E Tests</h3>
                      <p>User Journeys</p>
                  </div>
                  <div class="card">
                      <h3>Accessibility</h3>
                      <p>WCAG 2.2 AA+</p>
                  </div>
                  <div class="card">
                      <h3>Performance</h3>
                      <p>Core Web Vitals</p>
                  </div>
                  <div class="card">
                      <h3>Security</h3>
                      <p>Vulnerability Scan</p>
                  </div>
              </div>
              
              <div class="section">
                  <h2>ðŸ“Š Test Results Summary</h2>
                  <p>Detailed test results and artifacts are available in the individual test report files.</p>
              </div>
          </body>
          </html>
          EOF

      - name: Upload Comprehensive Test Report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: |
            test-reports/
            all-test-results/

      - name: Deploy Test Report to GitHub Pages
        if: github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./test-reports
          destination_dir: test-reports

# Environment-specific configuration
env:
  # Test configuration
  NODE_ENV: test
  CI: true
  
  # Database configuration for tests
  DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
  REDIS_URL: redis://localhost:6379
  
  # Application URLs for testing
  BASE_URL: http://localhost:3000
  API_BASE_URL: http://localhost:4000
  
  # Test timeouts
  TEST_TIMEOUT: 60000
  E2E_TIMEOUT: 120000
  
  # Performance thresholds
  PERFORMANCE_BUDGET_LCP: 2500
  PERFORMANCE_BUDGET_FID: 100
  PERFORMANCE_BUDGET_CLS: 0.1